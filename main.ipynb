{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import *\n",
    "from keras import Model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\"0\": \":heart:\",\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = extract_X_Y('./data/train_emoji.csv')\n",
    "X_test, Y_test = extract_X_Y('./data/test_emoji.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_Y_train = one_hot(Y_train.values, 5)\n",
    "oh_Y_test = one_hot(Y_test.values, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train Shape:\", X_train.shape)\n",
    "print(\"Y_train Shape:\", oh_Y_train.shape)\n",
    "print(\"X_test Shape:\", X_test.shape)\n",
    "print(\"Y_test Shape:\", oh_Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "print(X_train[idx], label_to_emoji(str(Y_train[idx]), emoji_dictionary))\n",
    "print(\"Label index %d is one-hot encoded as:\" % Y_train[idx], oh_Y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Emojifier V-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Emojifier V-1](./images/Emojifier-V1.png)\n",
    "\n",
    "#### Inputs and outputs\n",
    "* The input of the model is a string corresponding to a sentence (e.g. \"I love you). \n",
    "* The output will be a probability vector of shape (1,5), (there are 5 emojis to choose from).\n",
    "* The (1,5) probability vector is passed to an argmax layer, which extracts the index of the emoji with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Word Vector\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/) is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read GloVe File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_index, index_to_words, word_to_vec_map = read_glove_vecs('./GloVe/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word_to_vector map Shape:\", len(word_to_vec_map.get('food')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'food'\n",
    "word_idx = words_to_index.get(word)\n",
    "\n",
    "print(\"Word '%s' has index %d in GloVe vector.\" % (word, word_idx))\n",
    "\n",
    "idx = 250000\n",
    "word = index_to_words.get(idx)\n",
    "print(\"Index %d belong to the word '%s'.\" % (idx, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the V1 model\n",
    "\n",
    "Now that the sentences's average function is created and GloVe word vector is loaded, its time to construct the model. In this case the model uses cross-entropy cost function:\n",
    "\n",
    "$$ z^{(i)} = W . avg^{(i)} + b$$\n",
    "\n",
    "$$ a^{(i)} = softmax(z^{(i)})$$\n",
    "\n",
    "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Y_{oh,k}^{(i)} * log(a^{(i)}_k)$$\n",
    "\n",
    "And the gradients are computed as:\n",
    "\n",
    "$$ \\frac{d}{dx}Z^{(i)} = a^{(i)} - Y_{oh}^{(i)}$$\n",
    "\n",
    "$$ \\frac{d}{dx}W^{(i)} = \\frac{d}{dx}Z{(i)} . avg^{(i)}$$\n",
    "\n",
    "$$ \\frac{d}{dx}b^{(i)} = \\frac{d}{dx}Z^{(i)}$$\n",
    "\n",
    "***Note:*** The $Y_{oh}$ denotes one-hoted $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojifierV1:\n",
    "    def __init__(self, word_to_vector_map, lr=0.01, epochs=400, verbose=1):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.word_to_vector_map = word_to_vector_map\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        assert str(type(X)) == \"<class 'numpy.ndarray'>\"\n",
    "        costs = []\n",
    "        m = X.shape[0]                          \n",
    "        n_y = 5                                 \n",
    "        n_h = len(word_to_vec_map.get('food'))\n",
    "\n",
    "        W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "        b = np.zeros((n_y,))\n",
    "        \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        Y_oh = one_hot(Y, n_y) \n",
    "\n",
    "        for t in range(self.epochs):\n",
    "            for i in range(m):\n",
    "                avg = sentence_to_avg(X[i], self.word_to_vector_map)\n",
    "\n",
    "                z = np.matmul(self.W, avg) + self.b\n",
    "                a = softmax(z)\n",
    "\n",
    "                cost = -(np.matmul(Y_oh[i], np.log(a)))\n",
    "                \n",
    "                costs.append(cost)\n",
    "                \n",
    "                dz = a - Y_oh[i]\n",
    "                dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "                db = dz\n",
    "\n",
    "                self.W = self.W - self.lr * dW\n",
    "                self.b = self.b - self.lr * db\n",
    "\n",
    "            if self.verbose and t % 100 == 0:\n",
    "                print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "        return costs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        m = X.shape[0]\n",
    "        pred = np.zeros((m, 1))\n",
    "\n",
    "        for j in range(m):\n",
    "            avg = sentence_to_avg(X[j], self.word_to_vector_map)\n",
    "\n",
    "            Z = np.dot(self.W, avg) + self.b\n",
    "            A = softmax(Z)\n",
    "            pred[j] = np.argmax(A)\n",
    "        return pred\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        pred = self.predict(X)\n",
    "        accuracy = np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(type(X_train)) != \"<class 'numpy.ndarray'>\":\n",
    "    X_train = X_train.values\n",
    "    Y_train = Y_train.values\n",
    "    X_test = X_test.values\n",
    "    Y_test = Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmojifierV1(word_to_vec_map)\n",
    "cost_list = model.fit(X_train, Y_train)\n",
    "accuracy = model.evaluate(X_train, Y_train)\n",
    "print(\"Accuracy: %.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model.evaluate(X_train, Y_train)\n",
    "print(\"Training set accuracy: %.5f\" % train_acc)\n",
    "test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test set accuracy: %.5f' % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(sentences, labels, emoji_dictionary):\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        print(sentence, label_to_emoji(str(labels[idx]), emoji_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([\"0\", \"0\", \"2\", \"1\", \"4\", \"3\"])\n",
    "\n",
    "predictions = model.predict(X_my_sentences)\n",
    "print_predictions(X_my_sentences, Y_my_labels, emoji_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = np.array([\"I do not like you\"])\n",
    "prediction = model.predict(sentence)\n",
    "prediction = list(str(int(prediction[0][0])))\n",
    "print_predictions(sentence, prediction[0][0], emoji_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! \n",
    "* Because *adore* has a similar embedding as *love*, the algorithm has generalized correctly even to a word it has never seen before. \n",
    "* Words such as *heart*, *dear*, *beloved* or *adore* have embedding vectors similar to *love*. \n",
    "\n",
    "#### Word ordering isn't considered in this model\n",
    "* Note that the model doesn't get the following sentence correct:\n",
    ">I do not like you ❤\n",
    "\n",
    "* And it predicts the lable same as:\n",
    ">I love you ❤\n",
    "\n",
    "* This algorithm ignores word ordering, so is not good at understanding phrases like \"I do not like you\" \n",
    "\n",
    "#### Confusion matrix\n",
    "* Printing the confusion matrix can also help understand which classes are more difficult for your model. \n",
    "* A confusion matrix shows how often an example whose label is one class (\"actual\" class) is mislabeled by the algorithm with a different class (\"predicted\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    \n",
    "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "    \n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test.shape)\n",
    "pred_test = model.predict(X_test)\n",
    "print('            ' + label_to_emoji(\"0\", emoji_dictionary)+ '    ' + label_to_emoji(\"1\", emoji_dictionary) + '    ' +  label_to_emoji(\"2\", emoji_dictionary)+ '    ' + label_to_emoji(\"3\", emoji_dictionary)+'   ' + label_to_emoji(\"4\", emoji_dictionary))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(55,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1 Model Conclution\n",
    "- Even with a 127 training examples, you can get a reasonably good model for Emojifying. \n",
    "    - This is due to the generalization power word vectors provides. \n",
    "- Emojify-V1 will perform poorly on sentences such as **\"This movie is not good and not enjoyable\"**\n",
    "    - It doesn't understand combinations of words.\n",
    "    - It just averages all the words' embedding vectors together, without considering the ordering of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojifier-V2: Using LSTMs in Keras: \n",
    "\n",
    "Let's build an LSTM model that takes word **sequences** as input!\n",
    "* This model will be able to account for the word ordering. \n",
    "* Emojifier-V2 will continue to use pre-trained word embeddings to represent words.\n",
    "* We will feed word embeddings into an LSTM.\n",
    "* The LSTM will learn to predict the most appropriate emoji. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Emojifier-V2](./images/emojifier-v2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0] \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = [word.lower().replace('\\t', '') for word in X[i].split(' ') if word.replace('\\t', '') != '']\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j += 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1, words_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1 \n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "print(np.asarray(embedding_layer.get_weights()).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojifier-V2 (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2_Keras(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=[sentence_indices], outputs=X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())\n",
    "X_train_indices = sentences_to_indices(X_train, words_to_index, maxLen)\n",
    "Y_train_oh = one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Emojify_V2_Keras((maxLen,), word_to_vec_map, words_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mislabeled sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_oh = one_hot(Y_test, 5)\n",
    "X_test_indices = sentences_to_indices(X_test, words_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji: ' + label_to_emoji(str(Y_test[i]), emoji_dictionary) + '\\n\\tprediction: '+ X_test[i] + label_to_emoji(str(num), emoji_dictionary).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your own sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(['I need help'])\n",
    "X_test_indices = sentences_to_indices(x_test, words_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(str(np.argmax(model.predict(X_test_indices))), emoji_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
